{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/a/h/jsendi01/Envs/deep-venv/lib/python2.7/site-packages/sklearn/externals/joblib/_multiprocessing_helpers.py:28: UserWarning: [Errno 13] Permission denied.  joblib will operate in serial mode\n",
      "  warnings.warn('%s.  joblib will operate in serial mode' % (e,))\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "import keras\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import glob\n",
    "import os\n",
    "from random import shuffle\n",
    "from scipy.misc import imread\n",
    "from scipy.misc import imresize\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from ssd import SSD300\n",
    "from ssd_training import MultiboxLoss\n",
    "from ssd_utils import BBoxUtility\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (8, 8)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "# set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some constants\n",
    "INPUT_WIDTH = 300\n",
    "INPUT_HEIGHT = 300 \n",
    "NUM_CLASSES = 8 + 1 # 1 because of background (needed for training)\n",
    "DATA_PATH = '/a/data/fisheries_monitoring/data/'\n",
    "\n",
    "input_shape = (INPUT_HEIGHT, INPUT_WIDTH, 3)\n",
    "classes = ['ALB', 'DOL', 'NoF', 'SHARK', 'BET', 'LAG', 'OTHER', 'YFT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "priors = pickle.load(open('prior_boxes_ssd300.pkl', 'rb'))\n",
    "bbox_util = BBoxUtility(NUM_CLASSES, priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0 \tfolder name: /a/data/fisheries_monitoring/data/localizers/splice\n",
      "index: 1 \tfolder name: /a/data/fisheries_monitoring/data/localizers/invert\n",
      "index: 2 \tfolder name: /a/data/fisheries_monitoring/data/localizers/vflip\n",
      "index: 3 \tfolder name: /a/data/fisheries_monitoring/data/localizers/add\n",
      "index: 4 \tfolder name: /a/data/fisheries_monitoring/data/localizers/emboss\n",
      "index: 5 \tfolder name: /a/data/fisheries_monitoring/data/localizers/gaussianNoise\n",
      "index: 6 \tfolder name: /a/data/fisheries_monitoring/data/localizers/blur\n",
      "index: 7 \tfolder name: /a/data/fisheries_monitoring/data/localizers/original\n",
      "index: 8 \tfolder name: /a/data/fisheries_monitoring/data/localizers/dropout\n",
      "index: 9 \tfolder name: /a/data/fisheries_monitoring/data/localizers/rotate\n"
     ]
    }
   ],
   "source": [
    "aug_folders = glob.glob(DATA_PATH + 'localizers/*')\n",
    "for i, folder in enumerate(aug_folders):\n",
    "    print \"index:\", i, '\\t', \"folder name:\", folder\n",
    "    \n",
    "aug_folders = aug_folders[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def data_generator(batch_size, bbox_util, boxes, classes, INPUT_WIDTH, INPUT_HEIGHT):\n",
    "    # Shuffle labels to ensure \n",
    "    labels = shuffle(boxes)\n",
    "    while True:\n",
    "        #img_batch = np.zeros((batch_size, INPUT_HEIGHT, INPUT_WIDTH, 3))\n",
    "        #box_batch = np.zeros((batch_size, 4))\n",
    "        img_batch = []\n",
    "        target_batch = []\n",
    "        for index, row in boxes.iterrows():\n",
    "            # Get next file to read\n",
    "            file_name = row[\"img\"]\n",
    "            \n",
    "            # Obtain label for that file (remember file path in pattern aug_type/label/frame.png)\n",
    "            label = file_name.split('/')[1]\n",
    "            # Then create a one-hot encoding vector for the classes\n",
    "            label_vector = [1 if i == label else 0 for i in classes]\n",
    "\n",
    "            # Read img and preprocess it\n",
    "            path = DATA_PATH + 'localizers/' + file_name\n",
    "            img = image.load_img(path)\n",
    "            width, height = img.size\n",
    "            img = img.resize((INPUT_HEIGHT, INPUT_WIDTH))\n",
    "            img = image.img_to_array(img)\n",
    "            img /= 255\n",
    "            img_batch.append(img)\n",
    "            \n",
    "            # Read box and normalize its measures\n",
    "            old_x, old_y, old_w, old_h = row[[\"x\",\"y\",\"w\",\"h\"]]\n",
    "            new_x = old_x / width\n",
    "            new_y = old_y / height\n",
    "            new_w = old_w / width\n",
    "            new_h = old_h / height\n",
    "            \n",
    "            target = [new_x, new_y, new_x + new_w, new_y + new_h] + label_vector\n",
    "            target = bbox_util.assign_boxes(np.array(target)[np.newaxis, :])\n",
    "            target_batch.append(target)\n",
    "            \n",
    "            if len(target_batch) == batch_size:\n",
    "                tmp_img_batch = np.array(img_batch)\n",
    "                tmp_target_batch = np.array(target_batch)\n",
    "                img_batch = []\n",
    "                target_batch = []\n",
    "                yield (tmp_img_batch, tmp_target_batch)\n",
    "                \n",
    "        \n",
    "def load_all_labels(aug_folders):\n",
    "    all_targets = pd.DataFrame(columns = [\"img\", \"x\",\"y\",\"w\",\"h\"])\n",
    "    for folder in aug_folders:\n",
    "        folder_name = os.path.basename(folder)\n",
    "        \n",
    "        print \"Loading data augmentation folder:\", folder_name\n",
    "        targets = pd.read_csv(folder + '/superboxes.csv', names = [\"img\", \"x\",\"y\",\"w\",\"h\"])\n",
    "        targets = targets.sort_values(by = \"img\")\n",
    "        targets[\"img\"] = folder_name + '/' + targets[\"img\"]\n",
    "        print \"Number of examples:\", len(targets)\n",
    "        print\n",
    "        \n",
    "        all_targets = all_targets.append(targets)\n",
    "    print \"total number of examples: \", len(all_targets)\n",
    "    return all_targets\n",
    "\n",
    "\n",
    "def train_val_test_split(all_labels, val_size, test_size):\n",
    "    all_labels = shuffle(all_labels)\n",
    "    test_labels = all_labels[0:test_size]\n",
    "    val_labels = all_labels[test_size:test_size + val_size]\n",
    "    train_labels = all_labels[test_size + val_size:]\n",
    "    return train_labels, val_labels, test_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data augmentation folder: invert\n",
      "Number of examples: 6620\n",
      "\n",
      "Loading data augmentation folder: vflip\n",
      "Number of examples: 13240\n",
      "\n",
      "Loading data augmentation folder: add\n",
      "Number of examples: 6620\n",
      "\n",
      "Loading data augmentation folder: emboss\n",
      "Number of examples: 3310\n",
      "\n",
      "Loading data augmentation folder: gaussianNoise\n",
      "Number of examples: 9930\n",
      "\n",
      "Loading data augmentation folder: blur\n",
      "Number of examples: 6620\n",
      "\n",
      "Loading data augmentation folder: original\n",
      "Number of examples: 3310\n",
      "\n",
      "Loading data augmentation folder: dropout\n",
      "Number of examples: 6620\n",
      "\n",
      "Loading data augmentation folder: rotate\n",
      "Number of examples: 23170\n",
      "\n",
      "total number of examples:  79440\n"
     ]
    }
   ],
   "source": [
    "#gt = pickle.load(open('gt_pascal.pkl', 'rb'))\n",
    "#keys = sorted(gt.keys())\n",
    "#num_train = int(round(0.8 * len(keys)))\n",
    "#train_keys = keys[:num_train]\n",
    "#val_keys = keys[num_train:]\n",
    "#num_val = len(val_keys)\n",
    "\n",
    "all_labels = load_all_labels(aug_folders)\n",
    "\n",
    "train_labels, val_labels, test_labels = train_val_test_split(all_labels, 2000, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 30\n",
    "train_steps = np.ceil(len(train_labels)/batch_size)\n",
    "\n",
    "gen = data_generator(batch_size, bbox_util, train_labels, classes, INPUT_HEIGHT, INPUT_WIDTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = SSD300(input_shape, num_classes=NUM_CLASSES)\n",
    "model.load_weights(DATA_PATH + 'models/localizers/weights_SSD300.hdf5', by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freeze = ['input_1', 'conv1_1', 'conv1_2', 'pool1',\n",
    "          'conv2_1', 'conv2_2', 'pool2',\n",
    "          'conv3_1', 'conv3_2', 'conv3_3', 'pool3']#,\n",
    "#           'conv4_1', 'conv4_2', 'conv4_3', 'pool4']\n",
    "\n",
    "for L in model.layers:\n",
    "    if L.name in freeze:\n",
    "        L.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def schedule(epoch, decay=0.9):\n",
    "    return base_lr * decay**(epoch)\n",
    "\n",
    "callbacks = [keras.callbacks.ModelCheckpoint('./checkpoints/weights.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "                                             verbose=1,\n",
    "                                             save_weights_only=True),\n",
    "             keras.callbacks.LearningRateScheduler(schedule)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_lr = 3e-4\n",
    "optim = keras.optimizers.Adam(lr=base_lr)\n",
    "# optim = keras.optimizers.RMSprop(lr=base_lr)\n",
    "# optim = keras.optimizers.SGD(lr=base_lr, momentum=0.9, decay=decay, nesterov=True)\n",
    "model.compile(optimizer=optim,\n",
    "              loss=MultiboxLoss(NUM_CLASSES, neg_pos_ratio=2.0).compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_epoch = 30\n",
    "val_gen = data_generator(batch_size, bbox_util, val_labels, classes, INPUT_WIDTH, INPUT_HEIGHT)\n",
    "val_steps = np.ceil(len(val_labels)/batch_size)\n",
    "\n",
    "history = model.fit_generator(gen, len(train_labels),\n",
    "                              nb_epoch, verbose=2,\n",
    "                              callbacks=callbacks,\n",
    "                              validation_data=val_gen,\n",
    "                              nb_val_samples=len(val_labels),\n",
    "                              nb_worker=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs = []\n",
    "images = []\n",
    "img_path = path_prefix + sorted(val_keys)[0]\n",
    "img = image.load_img(img_path, target_size=(300, 300))\n",
    "img = image.img_to_array(img)\n",
    "images.append(imread(img_path))\n",
    "inputs.append(img.copy())\n",
    "inputs = preprocess_input(np.array(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = model.predict(inputs, batch_size=1, verbose=1)\n",
    "results = bbox_util.detection_out(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, img in enumerate(images):\n",
    "    # Parse the outputs.\n",
    "    det_label = results[i][:, 0]\n",
    "    det_conf = results[i][:, 1]\n",
    "    det_xmin = results[i][:, 2]\n",
    "    det_ymin = results[i][:, 3]\n",
    "    det_xmax = results[i][:, 4]\n",
    "    det_ymax = results[i][:, 5]\n",
    "\n",
    "    # Get detections with confidence higher than 0.6.\n",
    "    top_indices = [i for i, conf in enumerate(det_conf) if conf >= 0.6]\n",
    "\n",
    "    top_conf = det_conf[top_indices]\n",
    "    top_label_indices = det_label[top_indices].tolist()\n",
    "    top_xmin = det_xmin[top_indices]\n",
    "    top_ymin = det_ymin[top_indices]\n",
    "    top_xmax = det_xmax[top_indices]\n",
    "    top_ymax = det_ymax[top_indices]\n",
    "\n",
    "    colors = plt.cm.hsv(np.linspace(0, 1, 4)).tolist()\n",
    "\n",
    "    plt.imshow(img / 255.)\n",
    "    currentAxis = plt.gca()\n",
    "\n",
    "    for i in range(top_conf.shape[0]):\n",
    "        xmin = int(round(top_xmin[i] * img.shape[1]))\n",
    "        ymin = int(round(top_ymin[i] * img.shape[0]))\n",
    "        xmax = int(round(top_xmax[i] * img.shape[1]))\n",
    "        ymax = int(round(top_ymax[i] * img.shape[0]))\n",
    "        score = top_conf[i]\n",
    "        label = int(top_label_indices[i])\n",
    "#         label_name = voc_classes[label - 1]\n",
    "        display_txt = '{:0.2f}, {}'.format(score, label)\n",
    "        coords = (xmin, ymin), xmax-xmin+1, ymax-ymin+1\n",
    "        color = colors[label]\n",
    "        currentAxis.add_patch(plt.Rectangle(*coords, fill=False, edgecolor=color, linewidth=2))\n",
    "        currentAxis.text(xmin, ymin, display_txt, bbox={'facecolor':color, 'alpha':0.5})\n",
    "    \n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/a/h/jsendi01/Envs/deep-venv/lib/python2.7/site-packages/sklearn/externals/joblib/_multiprocessing_helpers.py:28: UserWarning: [Errno 13] Permission denied.  joblib will operate in serial mode\n",
      "  warnings.warn('%s.  joblib will operate in serial mode' % (e,))\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "import keras\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import glob\n",
    "import os\n",
    "from random import shuffle\n",
    "from scipy.misc import imread\n",
    "from scipy.misc import imresize\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from ssd import SSD300\n",
    "from ssd_training import MultiboxLoss\n",
    "from ssd_utils import BBoxUtility\n",
    "from data_utils import *\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (8, 8)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "# set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some constants\n",
    "INPUT_WIDTH = 300\n",
    "INPUT_HEIGHT = 300 \n",
    "NUM_CLASSES = 8 + 1 # 1 because of background (needed for training)\n",
    "DATA_PATH = '/a/data/fisheries_monitoring/data/'\n",
    "\n",
    "input_shape = (INPUT_HEIGHT, INPUT_WIDTH, 3)\n",
    "classes = ['ALB', 'DOL', 'NoF', 'SHARK', 'BET', 'LAG', 'OTHER', 'YFT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "priors = pickle.load(open('prior_boxes_ssd300.pkl', 'rb'))\n",
    "bbox_util = BBoxUtility(NUM_CLASSES, priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0 \tfolder name: /a/data/fisheries_monitoring/data/localizers/splice\n",
      "index: 1 \tfolder name: /a/data/fisheries_monitoring/data/localizers/invert\n",
      "index: 2 \tfolder name: /a/data/fisheries_monitoring/data/localizers/vflip\n",
      "index: 3 \tfolder name: /a/data/fisheries_monitoring/data/localizers/add\n",
      "index: 4 \tfolder name: /a/data/fisheries_monitoring/data/localizers/emboss\n",
      "index: 5 \tfolder name: /a/data/fisheries_monitoring/data/localizers/gaussianNoise\n",
      "index: 6 \tfolder name: /a/data/fisheries_monitoring/data/localizers/blur\n",
      "index: 7 \tfolder name: /a/data/fisheries_monitoring/data/localizers/original\n",
      "index: 8 \tfolder name: /a/data/fisheries_monitoring/data/localizers/dropout\n",
      "index: 9 \tfolder name: /a/data/fisheries_monitoring/data/localizers/rotate\n"
     ]
    }
   ],
   "source": [
    "aug_folders = glob.glob(DATA_PATH + 'localizers/*')\n",
    "for i, folder in enumerate(aug_folders):\n",
    "    print \"index:\", i, '\\t', \"folder name:\", folder\n",
    "    \n",
    "aug_folders = aug_folders[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def data_generator(batch_size, bbox_util, boxes, classes, INPUT_WIDTH, INPUT_HEIGHT):\n",
    "    while True:\n",
    "        #img_batch = np.zeros((batch_size, INPUT_HEIGHT, INPUT_WIDTH, 3))\n",
    "        #box_batch = np.zeros((batch_size, 4))\n",
    "        img_batch = []\n",
    "        target_batch = []\n",
    "        for file_name, value in boxes.iteritems():\n",
    "            # Obtain label for that file (remember file path in pattern aug_type/label/frame.png)\n",
    "            label = file_name.split('/')[1]\n",
    "            # Then create a one-hot encoding vector for the classes\n",
    "            label_vector = [1 if i == label else 0 for i in classes]\n",
    "\n",
    "            # Read img and preprocess it\n",
    "            path = DATA_PATH + 'localizers/' + file_name\n",
    "            img = image.load_img(path)\n",
    "                \n",
    "            width, height = img.size\n",
    "            img = img.resize((INPUT_HEIGHT, INPUT_WIDTH))\n",
    "            img = image.img_to_array(img)\n",
    "            img /= 255\n",
    "            img_batch.append(img)\n",
    "            \n",
    "            #target = bbox_util.assign_boxes(target)\n",
    "            old_x, old_y, old_w, old_h = value\n",
    "            new_x = old_x / width\n",
    "            new_y = old_y / height\n",
    "            new_w = old_w / width\n",
    "            new_h = old_h / height\n",
    "            \n",
    "            target = [new_x, new_y, new_x + new_w, new_y + new_h] + label_vector\n",
    "            target = bbox_util.assign_boxes(np.array(target)[np.newaxis, :])\n",
    "            \n",
    "            target_batch.append(target)\n",
    "            \n",
    "            if len(target_batch) == batch_size:\n",
    "                tmp_img_batch = np.array(img_batch)\n",
    "                tmp_target_batch = np.array(target_batch)\n",
    "                img_batch = []\n",
    "                target_batch = []\n",
    "                yield (tmp_img_batch, tmp_target_batch)\n",
    "                \n",
    "        \n",
    "def load_all_labels(aug_folders):\n",
    "    all_targets = pd.DataFrame(columns = [\"img\", \"x\",\"y\",\"w\",\"h\"])\n",
    "    for folder in aug_folders:\n",
    "        folder_name = os.path.basename(folder)\n",
    "        \n",
    "        print \"Loading data augmentation folder:\", folder_name\n",
    "        targets = pd.read_csv(folder + '/boxes.csv', names = [\"img\", \"x\",\"y\",\"w\",\"h\"])\n",
    "        targets = targets.sort_values(by = \"img\")\n",
    "        targets[\"id\"] = targets[\"img\"]\n",
    "        targets[\"img\"] = folder_name + '/' + targets[\"img\"]\n",
    "        print \"Number of examples:\", len(targets)\n",
    "        print\n",
    "        \n",
    "        all_targets = all_targets.append(targets)\n",
    "    print \"total number of examples: \", len(all_targets)\n",
    "    return all_targets\n",
    "\n",
    "def train_val_test_split(all_labels, val_size, test_size):\n",
    "    all_labels = shuffle(all_labels)\n",
    "    test_labels = all_labels[0:test_size]\n",
    "    val_labels = all_labels[test_size:test_size + val_size]\n",
    "    train_labels = all_labels[test_size + val_size:]\n",
    "    return train_labels, val_labels, test_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data augmentation folder: invert\n",
      "Number of examples: 8742\n",
      "\n",
      "Loading data augmentation folder: vflip\n",
      "Number of examples: 17484\n",
      "\n",
      "Loading data augmentation folder: add\n",
      "Number of examples: 8742\n",
      "\n",
      "Loading data augmentation folder: emboss\n",
      "Number of examples: 4371\n",
      "\n",
      "Loading data augmentation folder: gaussianNoise\n",
      "Number of examples: 13113\n",
      "\n",
      "Loading data augmentation folder: blur\n",
      "Number of examples: 8742\n",
      "\n",
      "Loading data augmentation folder: original\n",
      "Number of examples: 4371\n",
      "\n",
      "Loading data augmentation folder: dropout\n",
      "Number of examples: 8742\n",
      "\n",
      "Loading data augmentation folder: rotate\n",
      "Number of examples: 30597\n",
      "\n",
      "total number of examples:  104904\n"
     ]
    }
   ],
   "source": [
    "#gt = pickle.load(open('gt_pascal.pkl', 'rb'))\n",
    "#keys = sorted(gt.keys())\n",
    "#num_train = int(round(0.8 * len(keys)))\n",
    "#train_keys = keys[:num_train]\n",
    "#val_keys = keys[num_train:]\n",
    "#num_val = len(val_keys)\n",
    "\n",
    "all_labels = load_all_labels(aug_folders)\n",
    "train_labels, val_labels, test_labels = train_val_test_split(all_labels, 2000, 1000)\n",
    "\n",
    "train_labels = train_labels.set_index('img').T.to_dict('list')\n",
    "val_labels = val_labels.set_index('img').T.to_dict('list')\n",
    "test_labels = test_labels.set_index('img').T.to_dict('list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                img           x           y           w  \\\n",
      "0       invert/ALB/img_00003_15.jpg  377.000000   66.000000  353.000000   \n",
      "1       invert/ALB/img_00003_15.jpg  670.000000   95.000000  338.000000   \n",
      "2       invert/ALB/img_00003_15.jpg  820.000000  328.000000  303.000000   \n",
      "3       invert/ALB/img_00003_15.jpg  291.000000  122.000000  352.000000   \n",
      "4373    invert/ALB/img_00003_75.jpg  820.000000  328.000000  303.000000   \n",
      "4374    invert/ALB/img_00003_75.jpg  291.000000  122.000000  352.000000   \n",
      "4372    invert/ALB/img_00003_75.jpg  670.000000   95.000000  338.000000   \n",
      "4371    invert/ALB/img_00003_75.jpg  377.000000   66.000000  353.000000   \n",
      "5       invert/ALB/img_00010_15.jpg  831.000000  305.000000  112.000000   \n",
      "4       invert/ALB/img_00010_15.jpg  651.000000  422.000000   95.000000   \n",
      "4376    invert/ALB/img_00010_75.jpg  831.000000  305.000000  112.000000   \n",
      "4375    invert/ALB/img_00010_75.jpg  651.000000  422.000000   95.000000   \n",
      "6       invert/ALB/img_00012_15.jpg  471.000000  513.000000  156.000000   \n",
      "4377    invert/ALB/img_00012_75.jpg  471.000000  513.000000  156.000000   \n",
      "7       invert/ALB/img_00015_15.jpg  233.000000  341.000000  211.000000   \n",
      "4378    invert/ALB/img_00015_75.jpg  233.000000  341.000000  211.000000   \n",
      "8       invert/ALB/img_00019_15.jpg  155.000000  393.000000  138.000000   \n",
      "4379    invert/ALB/img_00019_75.jpg  155.000000  393.000000  138.000000   \n",
      "9       invert/ALB/img_00020_15.jpg  586.000000  537.000000  124.000000   \n",
      "10      invert/ALB/img_00020_15.jpg  690.000000  454.000000  157.000000   \n",
      "11      invert/ALB/img_00020_15.jpg  614.000000  377.000000  142.000000   \n",
      "12      invert/ALB/img_00020_15.jpg  724.000000  360.000000   92.000000   \n",
      "13      invert/ALB/img_00020_15.jpg  630.000000  327.000000  126.000000   \n",
      "4383    invert/ALB/img_00020_75.jpg  724.000000  360.000000   92.000000   \n",
      "4384    invert/ALB/img_00020_75.jpg  630.000000  327.000000  126.000000   \n",
      "4380    invert/ALB/img_00020_75.jpg  586.000000  537.000000  124.000000   \n",
      "4382    invert/ALB/img_00020_75.jpg  614.000000  377.000000  142.000000   \n",
      "4381    invert/ALB/img_00020_75.jpg  690.000000  454.000000  157.000000   \n",
      "15      invert/ALB/img_00029_15.jpg  693.000000  331.000000   80.000000   \n",
      "14      invert/ALB/img_00029_15.jpg  607.000000  343.000000   99.000000   \n",
      "...                             ...         ...         ...         ...   \n",
      "13749   rotate/YFT/img_07854_45.jpg  278.000000  148.000000  235.000000   \n",
      "636     rotate/YFT/img_07854_90.jpg  402.836879  775.744681  128.368794   \n",
      "21977  rotate/YFT/img_07891_135.jpg   22.000000    0.000000  694.000000   \n",
      "4493   rotate/YFT/img_07891_180.jpg  129.645390    6.028369  746.808511   \n",
      "17606  rotate/YFT/img_07891_225.jpg  448.000000    0.000000  619.000000   \n",
      "8864   rotate/YFT/img_07891_270.jpg    6.028369  403.546099  239.007092   \n",
      "26348  rotate/YFT/img_07891_315.jpg  564.000000  108.000000  694.000000   \n",
      "13235   rotate/YFT/img_07891_45.jpg  213.000000  299.000000  618.000000   \n",
      "122     rotate/YFT/img_07891_90.jpg  504.964539  129.645390  239.007092   \n",
      "22026  rotate/YFT/img_07901_135.jpg  707.000000    0.000000  331.000000   \n",
      "4542   rotate/YFT/img_07901_180.jpg  865.815603  237.234043  309.929078   \n",
      "17655  rotate/YFT/img_07901_225.jpg  783.000000  442.000000  331.000000   \n",
      "8913   rotate/YFT/img_07901_270.jpg  237.234043  104.255319  163.829787   \n",
      "26397  rotate/YFT/img_07901_315.jpg  242.000000  518.000000  331.000000   \n",
      "13284   rotate/YFT/img_07901_45.jpg  166.000000    0.000000  331.000000   \n",
      "171     rotate/YFT/img_07901_90.jpg  348.936170  865.815603  163.829787   \n",
      "22447  rotate/YFT/img_07911_135.jpg  327.000000  466.000000  405.000000   \n",
      "22446  rotate/YFT/img_07911_135.jpg  799.000000  144.000000  376.000000   \n",
      "4963   rotate/YFT/img_07911_180.jpg  143.120567  390.212766  380.851064   \n",
      "4962   rotate/YFT/img_07911_180.jpg  717.588652  500.141844  371.631206   \n",
      "18076  rotate/YFT/img_07911_225.jpg  115.000000   33.000000  419.000000   \n",
      "18075  rotate/YFT/img_07911_225.jpg  480.000000  519.000000  316.000000   \n",
      "9334   rotate/YFT/img_07911_270.jpg  390.212766  756.028369  217.021277   \n",
      "9333   rotate/YFT/img_07911_270.jpg  500.141844  190.780142  165.957447   \n",
      "26818  rotate/YFT/img_07911_315.jpg  548.000000    0.000000  406.000000   \n",
      "26817  rotate/YFT/img_07911_315.jpg  105.000000  200.000000  376.000000   \n",
      "13704   rotate/YFT/img_07911_45.jpg  483.000000    0.000000  317.000000   \n",
      "13705   rotate/YFT/img_07911_45.jpg  746.000000  268.000000  419.000000   \n",
      "591     rotate/YFT/img_07911_90.jpg   53.900709  717.588652  165.957447   \n",
      "592     rotate/YFT/img_07911_90.jpg  112.765957  143.120567  217.021277   \n",
      "\n",
      "                h  \n",
      "0      107.000000  \n",
      "1      124.000000  \n",
      "2      157.000000  \n",
      "3      285.000000  \n",
      "4373   157.000000  \n",
      "4374   285.000000  \n",
      "4372   124.000000  \n",
      "4371   107.000000  \n",
      "5      113.000000  \n",
      "4      190.000000  \n",
      "4376   113.000000  \n",
      "4375   190.000000  \n",
      "6      190.000000  \n",
      "4377   190.000000  \n",
      "7       94.000000  \n",
      "4378    94.000000  \n",
      "8       75.000000  \n",
      "4379    75.000000  \n",
      "9      182.000000  \n",
      "10     233.000000  \n",
      "11     107.000000  \n",
      "12     177.000000  \n",
      "13     111.000000  \n",
      "4383   177.000000  \n",
      "4384   111.000000  \n",
      "4380   182.000000  \n",
      "4382   107.000000  \n",
      "4381   233.000000  \n",
      "15     154.000000  \n",
      "14     181.000000  \n",
      "...           ...  \n",
      "13749  234.000000  \n",
      "636    207.092199  \n",
      "21977  642.000000  \n",
      "4493   239.007092  \n",
      "17606  451.000000  \n",
      "8864   746.808511  \n",
      "26348  641.000000  \n",
      "13235  450.000000  \n",
      "122    746.808511  \n",
      "22026  232.000000  \n",
      "4542   163.829787  \n",
      "17655  307.000000  \n",
      "8913   309.929078  \n",
      "26397  231.000000  \n",
      "13284  308.000000  \n",
      "171    309.929078  \n",
      "22447  253.000000  \n",
      "22446  376.000000  \n",
      "4963   217.021277  \n",
      "4962   165.957447  \n",
      "18076  419.000000  \n",
      "18075  200.000000  \n",
      "9334   380.851064  \n",
      "9333   371.631206  \n",
      "26818  254.000000  \n",
      "26817  376.000000  \n",
      "13704  201.000000  \n",
      "13705  419.000000  \n",
      "591    371.631206  \n",
      "592    380.851064  \n",
      "\n",
      "[104904 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 30\n",
    "train_steps = np.ceil(len(train_labels)/batch_size)\n",
    "\n",
    "gen = data_generator(batch_size, bbox_util, train_labels, classes, INPUT_HEIGHT, INPUT_WIDTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = SSD300(input_shape, num_classes=NUM_CLASSES)\n",
    "model.load_weights(DATA_PATH + 'models/localizers/weights_SSD300.hdf5', by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freeze = ['input_1', 'conv1_1', 'conv1_2', 'pool1',\n",
    "          'conv2_1', 'conv2_2', 'pool2',\n",
    "          'conv3_1', 'conv3_2', 'conv3_3', 'pool3']#,\n",
    "#           'conv4_1', 'conv4_2', 'conv4_3', 'pool4']\n",
    "\n",
    "for L in model.layers:\n",
    "    if L.name in freeze:\n",
    "        L.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def schedule(epoch, decay=0.9):\n",
    "    return base_lr * decay**(epoch)\n",
    "\n",
    "callbacks = [keras.callbacks.ModelCheckpoint('./checkpoints/weights.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "                                             verbose=1,\n",
    "                                             save_weights_only=True),\n",
    "             keras.callbacks.LearningRateScheduler(schedule)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_lr = 3e-4\n",
    "optim = keras.optimizers.Adam(lr=base_lr)\n",
    "# optim = keras.optimizers.RMSprop(lr=base_lr)\n",
    "# optim = keras.optimizers.SGD(lr=base_lr, momentum=0.9, decay=decay, nesterov=True)\n",
    "model.compile(optimizer=optim,\n",
    "              loss=MultiboxLoss(NUM_CLASSES, neg_pos_ratio=2.0).compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/a/h/jsendi01/Envs/deep-venv/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-f8ae303905e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m                               \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_gen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                               \u001b[0mnb_val_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                               nb_worker=1)\n\u001b[0m",
      "\u001b[1;32m/a/h/jsendi01/Envs/deep-venv/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, initial_epoch)\u001b[0m\n\u001b[0;32m   1555\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   1556\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1557\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   1558\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1559\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/a/h/jsendi01/Envs/deep-venv/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1318\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1320\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1321\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/a/h/jsendi01/Envs/deep-venv/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   1941\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1942\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m-> 1943\u001b[1;33m                               feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m   1944\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/a/h/jsendi01/Envs/deep-venv/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/a/h/jsendi01/Envs/deep-venv/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/a/h/jsendi01/Envs/deep-venv/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/a/h/jsendi01/Envs/deep-venv/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/a/h/jsendi01/Envs/deep-venv/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nb_epoch = 30\n",
    "val_gen = data_generator(batch_size, bbox_util, val_labels, classes, INPUT_WIDTH, INPUT_HEIGHT)\n",
    "val_steps = np.ceil(len(val_labels)/batch_size)\n",
    "\n",
    "history = model.fit_generator(gen, len(train_labels),\n",
    "                              nb_epoch, verbose=2,\n",
    "                              callbacks=callbacks,\n",
    "                              validation_data=val_gen,\n",
    "                              nb_val_samples=len(val_labels),\n",
    "                              nb_worker=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs = []\n",
    "images = []\n",
    "img_path = path_prefix + sorted(val_keys)[0]\n",
    "img = image.load_img(img_path, target_size=(300, 300))\n",
    "img = image.img_to_array(img)\n",
    "images.append(imread(img_path))\n",
    "inputs.append(img.copy())\n",
    "inputs = preprocess_input(np.array(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = model.predict(inputs, batch_size=1, verbose=1)\n",
    "results = bbox_util.detection_out(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, img in enumerate(images):\n",
    "    # Parse the outputs.\n",
    "    det_label = results[i][:, 0]\n",
    "    det_conf = results[i][:, 1]\n",
    "    det_xmin = results[i][:, 2]\n",
    "    det_ymin = results[i][:, 3]\n",
    "    det_xmax = results[i][:, 4]\n",
    "    det_ymax = results[i][:, 5]\n",
    "\n",
    "    # Get detections with confidence higher than 0.6.\n",
    "    top_indices = [i for i, conf in enumerate(det_conf) if conf >= 0.6]\n",
    "\n",
    "    top_conf = det_conf[top_indices]\n",
    "    top_label_indices = det_label[top_indices].tolist()\n",
    "    top_xmin = det_xmin[top_indices]\n",
    "    top_ymin = det_ymin[top_indices]\n",
    "    top_xmax = det_xmax[top_indices]\n",
    "    top_ymax = det_ymax[top_indices]\n",
    "\n",
    "    colors = plt.cm.hsv(np.linspace(0, 1, 4)).tolist()\n",
    "\n",
    "    plt.imshow(img / 255.)\n",
    "    currentAxis = plt.gca()\n",
    "\n",
    "    for i in range(top_conf.shape[0]):\n",
    "        xmin = int(round(top_xmin[i] * img.shape[1]))\n",
    "        ymin = int(round(top_ymin[i] * img.shape[0]))\n",
    "        xmax = int(round(top_xmax[i] * img.shape[1]))\n",
    "        ymax = int(round(top_ymax[i] * img.shape[0]))\n",
    "        score = top_conf[i]\n",
    "        label = int(top_label_indices[i])\n",
    "#         label_name = voc_classes[label - 1]\n",
    "        display_txt = '{:0.2f}, {}'.format(score, label)\n",
    "        coords = (xmin, ymin), xmax-xmin+1, ymax-ymin+1\n",
    "        color = colors[label]\n",
    "        currentAxis.add_patch(plt.Rectangle(*coords, fill=False, edgecolor=color, linewidth=2))\n",
    "        currentAxis.text(xmin, ymin, display_txt, bbox={'facecolor':color, 'alpha':0.5})\n",
    "    \n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/a/h/jsendi01/Envs/deep-venv/lib/python2.7/site-packages/sklearn/externals/joblib/_multiprocessing_helpers.py:28: UserWarning: [Errno 13] Permission denied.  joblib will operate in serial mode\n",
      "  warnings.warn('%s.  joblib will operate in serial mode' % (e,))\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "import keras\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import glob\n",
    "import os\n",
    "from random import shuffle\n",
    "from scipy.misc import imread\n",
    "from scipy.misc import imresize\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from ssd import SSD300\n",
    "from ssd_training import MultiboxLoss\n",
    "from ssd_utils import BBoxUtility\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (8, 8)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "# set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some constants\n",
    "INPUT_WIDTH = 300\n",
    "INPUT_HEIGHT = 300 \n",
    "NUM_CLASSES = 8\n",
    "DATA_PATH = '/a/data/fisheries_monitoring/data/'\n",
    "\n",
    "input_shape = (INPUT_HEIGHT, INPUT_WIDTH, 3)\n",
    "classes = ['ALB', 'DOL', 'NoF', 'SHARK', 'BET', 'LAG', 'OTHER', 'YFT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "priors = pickle.load(open('prior_boxes_ssd300.pkl', 'rb'))\n",
    "bbox_util = BBoxUtility(NUM_CLASSES, priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0 \tfolder name: /a/data/fisheries_monitoring/data/localizers/splice\n",
      "index: 1 \tfolder name: /a/data/fisheries_monitoring/data/localizers/invert\n",
      "index: 2 \tfolder name: /a/data/fisheries_monitoring/data/localizers/vflip\n",
      "index: 3 \tfolder name: /a/data/fisheries_monitoring/data/localizers/add\n",
      "index: 4 \tfolder name: /a/data/fisheries_monitoring/data/localizers/emboss\n",
      "index: 5 \tfolder name: /a/data/fisheries_monitoring/data/localizers/gaussianNoise\n",
      "index: 6 \tfolder name: /a/data/fisheries_monitoring/data/localizers/blur\n",
      "index: 7 \tfolder name: /a/data/fisheries_monitoring/data/localizers/original\n",
      "index: 8 \tfolder name: /a/data/fisheries_monitoring/data/localizers/dropout\n",
      "index: 9 \tfolder name: /a/data/fisheries_monitoring/data/localizers/rotate\n",
      "['/a/data/fisheries_monitoring/data/localizers/invert', '/a/data/fisheries_monitoring/data/localizers/vflip', '/a/data/fisheries_monitoring/data/localizers/add', '/a/data/fisheries_monitoring/data/localizers/emboss', '/a/data/fisheries_monitoring/data/localizers/gaussianNoise', '/a/data/fisheries_monitoring/data/localizers/blur', '/a/data/fisheries_monitoring/data/localizers/original', '/a/data/fisheries_monitoring/data/localizers/dropout', '/a/data/fisheries_monitoring/data/localizers/rotate']\n"
     ]
    }
   ],
   "source": [
    "aug_folders = glob.glob(DATA_PATH + 'localizers/*')\n",
    "for i, folder in enumerate(aug_folders):\n",
    "    print \"index:\", i, '\\t', \"folder name:\", folder\n",
    "    \n",
    "aug_folders = aug_folders[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def data_generator(batch_size, boxes, classes, INPUT_WIDTH, INPUT_HEIGHT):\n",
    "    # Shuffle labels to ensure \n",
    "    labels = shuffle(boxes)\n",
    "    while True:\n",
    "        #img_batch = np.zeros((batch_size, INPUT_HEIGHT, INPUT_WIDTH, 3))\n",
    "        #box_batch = np.zeros((batch_size, 4))\n",
    "        img_batch = []\n",
    "        target_batch = []\n",
    "        for index, row in boxes.iterrows():\n",
    "            # Get next file to read\n",
    "            file_name = row[\"img\"]\n",
    "            \n",
    "            # Obtain label for that file (remember file path in pattern aug_type/label/frame.png)\n",
    "            label = file_name.split('/')[1]\n",
    "            # Then create a one-hot encoding vector for the classes\n",
    "            label_vector = [1 if i == label else 0 for i in classes]\n",
    "\n",
    "            # Read img and preprocess it\n",
    "            path = DATA_PATH + 'localizers/' + file_name\n",
    "            img = image.load_img(path)\n",
    "            width, height = img.size\n",
    "            img = img.resize((INPUT_HEIGHT, INPUT_WIDTH))\n",
    "            img = image.img_to_array(img)\n",
    "            img /= 255\n",
    "            img_batch.append(img)\n",
    "            \n",
    "            # Read box and normalize its measures\n",
    "            old_x, old_y, old_w, old_h = row[[\"x\",\"y\",\"w\",\"h\"]]\n",
    "            new_x = old_x / width\n",
    "            new_y = old_y / height\n",
    "            new_w = old_w / width\n",
    "            new_h = old_h / height\n",
    "            \n",
    "            target = [new_x, new_y, new_x + new_w, new_y + new_h] + label_vector\n",
    "            target_batch.append(target)\n",
    "            \n",
    "            if len(target_batch) == batch_size:\n",
    "                yield (img_batch, target_batch)\n",
    "                \n",
    "        \n",
    "def load_all_labels(aug_folders):\n",
    "    all_targets = pd.DataFrame(columns = [\"img\", \"x\",\"y\",\"w\",\"h\"])\n",
    "    for folder in aug_folders:\n",
    "        folder_name = os.path.basename(folder)\n",
    "        \n",
    "        print \"Loading data augmentation folder:\", folder_name\n",
    "        targets = pd.read_csv(folder + '/superboxes.csv', names = [\"img\", \"x\",\"y\",\"w\",\"h\"])\n",
    "        targets = targets.sort_values(by = \"img\")\n",
    "        targets[\"img\"] = folder_name + '/' + targets[\"img\"]\n",
    "        print \"Number of examples:\", len(targets)\n",
    "        print\n",
    "        \n",
    "        all_targets = all_targets.append(targets)\n",
    "    print \"total number of examples: \", len(all_targets)\n",
    "    return all_targets\n",
    "\n",
    "\n",
    "def train_val_test_split(all_labels, val_size, test_size):\n",
    "    all_labels = shuffle(all_labels)\n",
    "    test_labels = all_labels[0:test_size]\n",
    "    val_labels = all_labels[test_size:test_size + val_size]\n",
    "    train_labels = all_labels[test_size + val_size:]\n",
    "    return train_labels, val_labels, test_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data augmentation folder: invert\n",
      "Number of examples: 6620\n",
      "\n",
      "Loading data augmentation folder: vflip\n",
      "Number of examples: 13240\n",
      "\n",
      "Loading data augmentation folder: add\n",
      "Number of examples: 6620\n",
      "\n",
      "Loading data augmentation folder: emboss\n",
      "Number of examples: 3310\n",
      "\n",
      "Loading data augmentation folder: gaussianNoise\n",
      "Number of examples: 9930\n",
      "\n",
      "Loading data augmentation folder: blur\n",
      "Number of examples: 6620\n",
      "\n",
      "Loading data augmentation folder: original\n",
      "Number of examples: 3310\n",
      "\n",
      "Loading data augmentation folder: dropout\n",
      "Number of examples: 6620\n",
      "\n",
      "Loading data augmentation folder: rotate\n",
      "Number of examples: 23170\n",
      "\n",
      "total number of examples:  79440\n"
     ]
    }
   ],
   "source": [
    "#gt = pickle.load(open('gt_pascal.pkl', 'rb'))\n",
    "#keys = sorted(gt.keys())\n",
    "#num_train = int(round(0.8 * len(keys)))\n",
    "#train_keys = keys[:num_train]\n",
    "#val_keys = keys[num_train:]\n",
    "#num_val = len(val_keys)\n",
    "\n",
    "all_labels = load_all_labels(aug_folders)\n",
    "\n",
    "train_labels, val_labels, test_labels = train_val_test_split(all_labels, 2000, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 30\n",
    "train_steps = np.ceil(len(train_labels)/batch_size)\n",
    "\n",
    "gen = data_generator(batch_size, train_labels, classes, INPUT_HEIGHT, INPUT_WIDTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ssd.py:40: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\", name=\"conv1_1\")`\n",
      "  name='conv1_1')(net['input'])\n",
      "ssd.py:44: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\", name=\"conv1_2\")`\n",
      "  name='conv1_2')(net['conv1_1'])\n",
      "ssd.py:46: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D((2, 2), padding=\"same\", strides=(2, 2), name=\"pool1\")`\n",
      "  name='pool1')(net['conv1_2'])\n",
      "ssd.py:51: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\", name=\"conv2_1\")`\n",
      "  name='conv2_1')(net['pool1'])\n",
      "ssd.py:55: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\", name=\"conv2_2\")`\n",
      "  name='conv2_2')(net['conv2_1'])\n",
      "ssd.py:57: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D((2, 2), padding=\"same\", strides=(2, 2), name=\"pool2\")`\n",
      "  name='pool2')(net['conv2_2'])\n",
      "ssd.py:62: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\", name=\"conv3_1\")`\n",
      "  name='conv3_1')(net['pool2'])\n",
      "ssd.py:66: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\", name=\"conv3_2\")`\n",
      "  name='conv3_2')(net['conv3_1'])\n",
      "ssd.py:70: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\", name=\"conv3_3\")`\n",
      "  name='conv3_3')(net['conv3_2'])\n",
      "ssd.py:72: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D((2, 2), padding=\"same\", strides=(2, 2), name=\"pool3\")`\n",
      "  name='pool3')(net['conv3_3'])\n",
      "ssd.py:77: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\", name=\"conv4_1\")`\n",
      "  name='conv4_1')(net['pool3'])\n",
      "ssd.py:81: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\", name=\"conv4_2\")`\n",
      "  name='conv4_2')(net['conv4_1'])\n",
      "ssd.py:85: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\", name=\"conv4_3\")`\n",
      "  name='conv4_3')(net['conv4_2'])\n",
      "ssd.py:87: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D((2, 2), padding=\"same\", strides=(2, 2), name=\"pool4\")`\n",
      "  name='pool4')(net['conv4_3'])\n",
      "ssd.py:92: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\", name=\"conv5_1\")`\n",
      "  name='conv5_1')(net['pool4'])\n",
      "ssd.py:96: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\", name=\"conv5_2\")`\n",
      "  name='conv5_2')(net['conv5_1'])\n",
      "ssd.py:100: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\", name=\"conv5_3\")`\n",
      "  name='conv5_3')(net['conv5_2'])\n",
      "ssd.py:102: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D((3, 3), padding=\"same\", strides=(1, 1), name=\"pool5\")`\n",
      "  name='pool5')(net['conv5_3'])\n",
      "/a/h/jsendi01/Envs/deep-venv/lib/python2.7/site-packages/keras/legacy/layers.py:753: UserWarning: The `AtrousConvolution2D` layer  has been deprecated. Use instead the `Conv2D` layer with the `dilation_rate` argument.\n",
      "  warnings.warn('The `AtrousConvolution2D` layer '\n",
      "/a/h/jsendi01/Envs/deep-venv/lib/python2.7/site-packages/keras/legacy/layers.py:757: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1024, (3, 3), padding=\"same\", activation=\"relu\", dilation_rate=(6, 6), name=\"fc6\")`\n",
      "  return Conv2D(*args, **kwargs)\n",
      "ssd.py:110: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1024, (1, 1), padding=\"same\", activation=\"relu\", name=\"fc7\")`\n",
      "  border_mode='same', name='fc7')(net['fc6'])\n",
      "ssd.py:115: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (1, 1), padding=\"same\", activation=\"relu\", name=\"conv6_1\")`\n",
      "  name='conv6_1')(net['fc7'])\n",
      "ssd.py:118: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), padding=\"same\", strides=(2, 2), activation=\"relu\", name=\"conv6_2\")`\n",
      "  name='conv6_2')(net['conv6_1'])\n",
      "ssd.py:122: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), padding=\"same\", activation=\"relu\", name=\"conv7_1\")`\n",
      "  name='conv7_1')(net['conv6_2'])\n",
      "ssd.py:126: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"valid\", strides=(2, 2), activation=\"relu\", name=\"conv7_2\")`\n",
      "  name='conv7_2')(net['conv7_2'])\n",
      "ssd.py:130: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), padding=\"same\", activation=\"relu\", name=\"conv8_1\")`\n",
      "  name='conv8_1')(net['conv7_2'])\n",
      "ssd.py:133: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", strides=(2, 2), activation=\"relu\", name=\"conv8_2\")`\n",
      "  name='conv8_2')(net['conv8_1'])\n",
      "ssd.py:140: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(12, (3, 3), padding=\"same\", name=\"conv4_3_norm_mbox_loc\")`\n",
      "  name='conv4_3_norm_mbox_loc')(net['conv4_3_norm'])\n",
      "ssd.py:148: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(24, (3, 3), padding=\"same\", name=\"conv4_3_norm_mbox_conf_8\")`\n",
      "  name=name)(net['conv4_3_norm'])\n",
      "ssd.py:160: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(24, (3, 3), padding=\"same\", name=\"fc7_mbox_loc\")`\n",
      "  name='fc7_mbox_loc')(net['fc7'])\n",
      "ssd.py:168: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(48, (3, 3), padding=\"same\", name=\"fc7_mbox_conf_8\")`\n",
      "  name=name)(net['fc7'])\n",
      "ssd.py:178: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(24, (3, 3), padding=\"same\", name=\"conv6_2_mbox_loc\")`\n",
      "  name='conv6_2_mbox_loc')(net['conv6_2'])\n",
      "ssd.py:186: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(48, (3, 3), padding=\"same\", name=\"conv6_2_mbox_conf_8\")`\n",
      "  name=name)(net['conv6_2'])\n",
      "ssd.py:197: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(24, (3, 3), padding=\"same\", name=\"conv7_2_mbox_loc\")`\n",
      "  name='conv7_2_mbox_loc')(net['conv7_2'])\n",
      "ssd.py:205: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(48, (3, 3), padding=\"same\", name=\"conv7_2_mbox_conf_8\")`\n",
      "  name=name)(net['conv7_2'])\n",
      "ssd.py:216: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(24, (3, 3), padding=\"same\", name=\"conv8_2_mbox_loc\")`\n",
      "  name='conv8_2_mbox_loc')(net['conv8_2'])\n",
      "ssd.py:224: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(48, (3, 3), padding=\"same\", name=\"conv8_2_mbox_conf_8\")`\n",
      "  name=name)(net['conv8_2'])\n",
      "ssd.py:258: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  mode='concat', concat_axis=1, name='mbox_loc')\n",
      "/a/h/jsendi01/Envs/deep-venv/lib/python2.7/site-packages/keras/legacy/layers.py:456: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "ssd.py:265: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  mode='concat', concat_axis=1, name='mbox_conf')\n",
      "ssd.py:273: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name='mbox_priorbox')\n",
      "ssd.py:288: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name='predictions')\n"
     ]
    }
   ],
   "source": [
    "model = SSD300(input_shape, num_classes=NUM_CLASSES)\n",
    "model.load_weights(DATA_PATH + 'models/localizers/weights_SSD300.hdf5', by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freeze = ['input_1', 'conv1_1', 'conv1_2', 'pool1',\n",
    "          'conv2_1', 'conv2_2', 'pool2',\n",
    "          'conv3_1', 'conv3_2', 'conv3_3', 'pool3']#,\n",
    "#           'conv4_1', 'conv4_2', 'conv4_3', 'pool4']\n",
    "\n",
    "for L in model.layers:\n",
    "    if L.name in freeze:\n",
    "        L.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def schedule(epoch, decay=0.9):\n",
    "    return base_lr * decay**(epoch)\n",
    "\n",
    "callbacks = [keras.callbacks.ModelCheckpoint('./checkpoints/weights.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "                                             verbose=1,\n",
    "                                             save_weights_only=True),\n",
    "             keras.callbacks.LearningRateScheduler(schedule)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_lr = 3e-4\n",
    "optim = keras.optimizers.Adam(lr=base_lr)\n",
    "# optim = keras.optimizers.RMSprop(lr=base_lr)\n",
    "# optim = keras.optimizers.SGD(lr=base_lr, momentum=0.9, decay=decay, nesterov=True)\n",
    "model.compile(optimizer=optim,\n",
    "              loss=MultiboxLoss(NUM_CLASSES, neg_pos_ratio=2.0).compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/a/h/jsendi01/Envs/deep-venv/lib/python2.7/site-packages/ipykernel/__main__.py:10: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., 2548.0, 30, verbose=1, workers=1, validation_data=<generator..., callbacks=[<keras.ca..., validation_steps=66.0)`\n",
      "/a/h/jsendi01/Envs/deep-venv/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 30\n",
    "val_gen = data_generator(batch_size, val_labels, classes, INPUT_WIDTH, INPUT_HEIGHT)\n",
    "val_steps = np.ceil(len(val_labels)/batch_size)\n",
    "\n",
    "history = model.fit_generator(gen, train_steps,\n",
    "                              nb_epoch, verbose=1,\n",
    "                              callbacks=callbacks,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps,\n",
    "                              nb_worker=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs = []\n",
    "images = []\n",
    "img_path = path_prefix + sorted(val_keys)[0]\n",
    "img = image.load_img(img_path, target_size=(300, 300))\n",
    "img = image.img_to_array(img)\n",
    "images.append(imread(img_path))\n",
    "inputs.append(img.copy())\n",
    "inputs = preprocess_input(np.array(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = model.predict(inputs, batch_size=1, verbose=1)\n",
    "results = bbox_util.detection_out(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, img in enumerate(images):\n",
    "    # Parse the outputs.\n",
    "    det_label = results[i][:, 0]\n",
    "    det_conf = results[i][:, 1]\n",
    "    det_xmin = results[i][:, 2]\n",
    "    det_ymin = results[i][:, 3]\n",
    "    det_xmax = results[i][:, 4]\n",
    "    det_ymax = results[i][:, 5]\n",
    "\n",
    "    # Get detections with confidence higher than 0.6.\n",
    "    top_indices = [i for i, conf in enumerate(det_conf) if conf >= 0.6]\n",
    "\n",
    "    top_conf = det_conf[top_indices]\n",
    "    top_label_indices = det_label[top_indices].tolist()\n",
    "    top_xmin = det_xmin[top_indices]\n",
    "    top_ymin = det_ymin[top_indices]\n",
    "    top_xmax = det_xmax[top_indices]\n",
    "    top_ymax = det_ymax[top_indices]\n",
    "\n",
    "    colors = plt.cm.hsv(np.linspace(0, 1, 4)).tolist()\n",
    "\n",
    "    plt.imshow(img / 255.)\n",
    "    currentAxis = plt.gca()\n",
    "\n",
    "    for i in range(top_conf.shape[0]):\n",
    "        xmin = int(round(top_xmin[i] * img.shape[1]))\n",
    "        ymin = int(round(top_ymin[i] * img.shape[0]))\n",
    "        xmax = int(round(top_xmax[i] * img.shape[1]))\n",
    "        ymax = int(round(top_ymax[i] * img.shape[0]))\n",
    "        score = top_conf[i]\n",
    "        label = int(top_label_indices[i])\n",
    "#         label_name = voc_classes[label - 1]\n",
    "        display_txt = '{:0.2f}, {}'.format(score, label)\n",
    "        coords = (xmin, ymin), xmax-xmin+1, ymax-ymin+1\n",
    "        color = colors[label]\n",
    "        currentAxis.add_patch(plt.Rectangle(*coords, fill=False, edgecolor=color, linewidth=2))\n",
    "        currentAxis.text(xmin, ymin, display_txt, bbox={'facecolor':color, 'alpha':0.5})\n",
    "    \n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
